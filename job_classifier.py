# -*- coding: utf-8 -*-
"""Job_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uobRpua0PlzD1kSokLCL2gb4NH76oMhS
"""


import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
import joblib
import os
from datetime import datetime

# Create output directory
os.makedirs("results", exist_ok=True)

# 1. Web Scraping Function
def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"üîç Scraping page: {page} for keyword: {keyword}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"‚ö†Ô∏è Error parsing job block: {e}")
                continue

        time.sleep(1)

    return pd.DataFrame(jobs_list)

# 2. Preprocessing
def preprocess_skills(df):
    df = df.copy()
    df["Skills"] = df["Skills"].fillna("").str.lower()
    vectorizer = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", stop_words="english")
    X = vectorizer.fit_transform(df["Skills"])
    return normalize(X), vectorizer

# 3. Clustering
def cluster_skills(X, n_clusters=5):
    model = KMeans(n_clusters=n_clusters, random_state=42)
    model.fit(X)
    return model

# 4. Save models
def save_model(model, vectorizer, model_path="job_cluster_model.pkl", vec_path="tfidf_vectorizer.pkl"):
    joblib.dump(model, model_path)
    joblib.dump(vectorizer, vec_path)

# 5. Classify New Jobs
def classify_new_jobs(new_df, model, vectorizer):
    new_df = new_df.copy()
    new_df["Skills"] = new_df["Skills"].fillna("").str.lower()
    X_new = vectorizer.transform(new_df["Skills"])
    new_df["Cluster"] = model.predict(normalize(X_new))
    return new_df

# 6. Notify User
def notify_user(new_df, user_preferred_clusters):
    matched_jobs = new_df[new_df["Cluster"].isin(user_preferred_clusters)]
    if not matched_jobs.empty:
        print("üö® New jobs found in your preferred categories:")
        print(matched_jobs[["Title", "Company", "Location", "Skills"]])
    else:
        print("‚úÖ No new jobs in your preferred categories today.")
    return matched_jobs

# 7. Daily Job Check
def run_daily_job_check(keyword, user_clusters, model, vectorizer):
    print(f"\nüîÑ Checking new jobs for: {keyword}")
    new_jobs = scrape_karkidi_jobs(keyword=keyword, pages=1)
    classified = classify_new_jobs(new_jobs, model, vectorizer)
    matched = notify_user(classified, user_clusters)

    # Save all scraped jobs
    today = datetime.now().strftime("%Y-%m-%d_%H-%M")
    all_jobs_path = f"results/{keyword}_all_jobs_{today}.csv"
    matched_jobs_path = f"results/{keyword}_matched_jobs_{today}.csv"

    classified.to_csv(all_jobs_path, index=False)
    matched.to_csv(matched_jobs_path, index=False)

    print(f"üìÅ Saved classified jobs to: {all_jobs_path}")
    print(f"üìÅ Saved matched jobs to: {matched_jobs_path}")

    return matched

