# -*- coding: utf-8 -*-
"""Job_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uobRpua0PlzD1kSokLCL2gb4NH76oMhS
"""


import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
import joblib
import os
from datetime import datetime

# Create output directory
os.makedirs("results", exist_ok=True)

# 1. Web Scraping Function
def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"üîç Scraping page: {page} for keyword: {keyword}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"‚ö†Ô∏è Error parsing job block: {e}")
                continue

        time.sleep(1)

    return pd.DataFrame(jobs_list)

# 2. Preprocessing
def preprocess_skills(df):
    df = df.copy()
    df["Skills"] = df["Skills"].fillna("").str.lower()
    vectorizer = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", stop_words="english")
    X = vectorizer.fit_transform(df["Skills"])
    return normalize(X), vectorizer

# 3. Clustering
def cluster_skills(X, n_clusters=5):
    model = KMeans(n_clusters=n_clusters, random_state=42)
    model.fit(X)
    return model

# 4. Save models
def save_model(model, vectorizer, model_path="job_cluster_model.pkl", vec_path="tfidf_vectorizer.pkl"):
    joblib.dump(model, model_path)
    joblib.dump(vectorizer, vec_path)

# 5. Classify New Jobs
def classify_new_jobs(new_df, model, vectorizer):
    new_df = new_df.copy()
    new_df["Skills"] = new_df["Skills"].fillna("").str.lower()
    X_new = vectorizer.transform(new_df["Skills"])
    new_df["Cluster"] = model.predict(normalize(X_new))
    return new_df

# 6. Notify User
def notify_user(new_df, user_preferred_clusters):
    matched_jobs = new_df[new_df["Cluster"].isin(user_preferred_clusters)]
    if not matched_jobs.empty:
        print("üö® New jobs found in your preferred categories:")
        print(matched_jobs[["Title", "Company", "Location", "Skills"]])
    else:
        print("‚úÖ No new jobs in your preferred categories today.")
    return matched_jobs

# 7. Daily Job Check
def run_daily_job_check(keyword, user_clusters, model, vectorizer):
    print(f"\nüîÑ Checking new jobs for: {keyword}")
    new_jobs = scrape_karkidi_jobs(keyword=keyword, pages=1)
    classified = classify_new_jobs(new_jobs, model, vectorizer)
    matched = notify_user(classified, user_clusters)

    # Save all scraped jobs
    today = datetime.now().strftime("%Y-%m-%d_%H-%M")
    all_jobs_path = f"results/{keyword}_all_jobs_{today}.csv"
    matched_jobs_path = f"results/{keyword}_matched_jobs_{today}.csv"

    classified.to_csv(all_jobs_path, index=False)
    matched.to_csv(matched_jobs_path, index=False)

    print(f"üìÅ Saved classified jobs to: {all_jobs_path}")
    print(f"üìÅ Saved matched jobs to: {matched_jobs_path}")

    return matched

# -------- MAIN FLOW --------
if __name__ == "__main__":
    # Step 1: Scrape & Save Base Jobs
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    df_jobs.to_csv("results/karkidi_jobs.csv", index=False)

    # Step 2: Preprocess & Cluster
    X_skills, tfidf_vectorizer = preprocess_skills(df_jobs)
    kmeans_model = cluster_skills(X_skills, n_clusters=5)
    save_model(kmeans_model, tfidf_vectorizer)

    # Step 3: Classify Original Jobs
    df_classified = classify_new_jobs(df_jobs, kmeans_model, tfidf_vectorizer)
    df_classified.to_csv("results/karkidi_classified_jobs.csv", index=False)

    # Step 4: Define User Interests
    interests = {
        "data science": [0, 4],
        "cloud": [1],
        "machine learning": [2]
    }

    # Step 5: Run Daily Checks
    for keyword, clusters in interests.items():
        run_daily_job_check(keyword, clusters, kmeans_model, tfidf_vectorizer)


# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # app.py
# 
# import streamlit as st
# import pandas as pd
# import joblib
# from job_recommender import scrape_karkidi_jobs, classify_new_jobs, notify_user
# from datetime import datetime
# from sklearn.preprocessing import normalize
# 
# # Load model and vectorizer
# model = joblib.load("job_cluster_model.pkl")
# vectorizer = joblib.load("tfidf_vectorizer.pkl")
# 
# # App title
# st.title("üíº Job Recommendation System")
# st.write("This app scrapes jobs from Karkidi.com, clusters them by required skills, and notifies users based on their selected interests.")
# 
# # Sidebar - Select keyword and cluster interests
# keyword = st.sidebar.selectbox("üîç Choose job keyword", ["data science", "cloud", "machine learning", "AI", "NLP"])
# user_clusters = st.sidebar.multiselect("üéØ Select preferred job clusters (0‚Äì4)", [0, 1, 2, 3, 4], default=[0, 2])
# 
# if st.sidebar.button("üöÄ Run Job Check"):
#     st.subheader(f"üîÑ Checking new jobs for: {keyword}")
#     new_jobs = scrape_karkidi_jobs(keyword=keyword, pages=1)
#     classified = classify_new_jobs(new_jobs, model, vectorizer)
#     matched = notify_user(classified, user_clusters)
# 
#     # Timestamp
#     now = datetime.now().strftime("%Y-%m-%d_%H-%M")
#     all_jobs_path = f"results/{keyword}_all_jobs_{now}.csv"
#     matched_jobs_path = f"results/{keyword}_matched_jobs_{now}.csv"
# 
#     classified.to_csv(all_jobs_path, index=False)
#     matched.to_csv(matched_jobs_path, index=False)
# 
#     # Display and Download
#     st.subheader("‚úÖ Classified Jobs")
#     st.dataframe(classified)
#     st.download_button("‚¨áÔ∏è Download All Classified Jobs", data=classified.to_csv(index=False), file_name="classified_jobs.csv")
# 
#     st.subheader("üö® Matched Jobs")
#     st.dataframe(matched)
#     if not matched.empty:
#         st.download_button("‚¨áÔ∏è Download Matched Jobs", data=matched.to_csv(index=False), file_name="matched_jobs.csv")
#     else:
#         st.info("No matched jobs found today.")
# else:
#     st.info("Click the button in the sidebar to check new job listings.")
